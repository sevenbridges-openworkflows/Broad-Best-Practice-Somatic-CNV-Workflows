$namespaces:
  sbg: https://sevenbridges.com
arguments:
- position: 0
  prefix: ''
  shellQuote: false
  valueFrom: /opt/gatk
- position: 1
  shellQuote: false
  valueFrom: --java-options
- position: 2
  prefix: ''
  shellQuote: false
  valueFrom: "${\n    if (inputs.memory_per_job) {\n        return '\\\"-Xmx'.concat(inputs.memory_per_job,\
    \ 'M') + '\\\"';\n    }\n    return '\\\"-Xmx2048M\\\"';\n}"
- position: 3
  shellQuote: false
  valueFrom: CreateReadCountPanelOfNormals
- position: 5
  prefix: --output
  shellQuote: false
  valueFrom: "${\n    var read_counts = [].concat(inputs.read_counts);\n    var prefix\
    \ = inputs.output_prefix ? inputs.output_prefix : read_counts[0].path.split('/').pop().split('.').slice(0,-1).join('.');\n\
    \    return prefix + '.pon.hdf5';\n}"
baseCommand: []
class: CommandLineTool
cwlVersion: v1.0
doc: "This app creates a panel of normals (PoN) for read-count denoising given the\
  \ read counts for samples in the panel. \n\n\n### Common Use Cases\n\nThis app produces\
  \ panel of normals file which can be used with **DenoiseReadCounts** to denoise\
  \ other samples. The input read counts are first transformed to log2 fractional\
  \ coverages and preprocessed according to specified filtering and imputation parameters.\
  \ Singular value decomposition (SVD) is then performed to find the first number-of-eigensamples\
  \ principal components, which are stored in the PoN. Some or all of these principal\
  \ components can then be used for denoising case samples with **DenoiseReadCounts**;\
  \ it is assumed that the principal components used represent systematic sequencing\
  \ biases (rather than statistical noise). Examining the singular values, which are\
  \ also stored in the PoN, may be useful in determining the appropriate number of\
  \ principal components to use for denoising.\n\nIf annotated intervals are provided,\
  \ explicit GC-bias correction will be performed by **GCBiasCorrector** before filtering\
  \ and SVD. GC-content information for the intervals will be stored in the PoN and\
  \ used to perform explicit GC-bias correction identically in **DenoiseReadCounts**.\
  \ Note that if annotated intervals are not provided, it is still likely that GC-bias\
  \ correction is implicitly performed by the SVD denoising process (i.e., some of\
  \ the principal components arise from GC bias).\n\nNote that such SVD denoising\
  \ cannot distinguish between variance due to systematic sequencing biases and that\
  \ due to true common germline CNVs present in the panel; signal from the latter\
  \ may thus be inadvertently denoised away. Furthermore, variance arising from coverage\
  \ on the sex chromosomes may also significantly contribute to the principal components\
  \ if the panel contains samples of mixed sex. Therefore, if sex chromosomes are\
  \ not excluded from coverage collection, it is strongly recommended that users avoid\
  \ creating panels of mixed sex and take care to denoise case samples only with panels\
  \ containing only individuals of the same sex as the case samples. (See **GermlineCNVCaller**,\
  \ which avoids these issues by simultaneously learning a probabilistic model for\
  \ systematic bias and calling rare and common germline CNVs for samples in the panel.)\n\
  \n*Source: [https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_copynumber_CreateReadCountPanelOfNormals.php](https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_copynumber_CreateReadCountPanelOfNormals.php)*\n\
  \nSome of the input parameters are listed below:\n* **Input read counts** (`--input`)\
  \  - TSV or HDF5 files containing read count data. These files are output of **CollectReadCounts**\
  \ tool.\n* **Annotated intervals** (`--annotated-intervals`) file from **AnnotateIntervals**.\
  \ Explicit GC-bias correction will be performed on the panel samples and identically\
  \ for subsequent case samples.\n* **Output prefix** (`--output`) - Also known as\
  \ *PoN entity ID* (per [CNV best practice WDL](https://github.com/gatk-workflows/gatk4-somatic-cnvs/blob/master/cnv_somatic_panel_workflow.wdl)\
  \ specification)\n\n### Changes Introduced by Seven Bridges\n* Some of the input\
  \ arguments which are not applicable to this tool have been removed (`--use-jdk-deflater`,\
  \ `--gcs-max-retries`, etc.)\n* Output panel of normals will be named by adding\
  \ extension `.pon.hdf5` to the output prefix provided through **Output prefix**\
  \ parameter. If this parameter is not set the output name will be generated by taking\
  \ base name of the first read counts file and appending the extension  `.pon.hdf5`\
  \ to it.\n\n### Common Issues and Important Notes\n* Default memory allocated for\
  \ execution is 2048 (Mb) which may be insufficient for processing larger number\
  \ of read count files. In this case please allocate more memory through **Memory\
  \ per job** input parameter.\n\n### Performance Benchmarking\n\n| Input Size | Experimental\
  \ Strategy | Duration | Cost (spot) | AWS Instance Type |\n| --- | --- | --- | ---\
  \ | --- | \n| 4x84MB | WGS | 3min | $0.02 | c4.2xlarge |\n| 8x84MB | WGS | 4min\
  \ | $0.02 | c4.2xlarge |\n| 16x84MB | WGS | 5min | $0.02 | c4.2xlarge |"
id: uros_sipetic/gatk-4-1-0-0-demo/gatk-createreadcountpanelofnormals-4-1-0-0/2
inputs:
- doc: Input file containing annotations for gc content in genomic intervals (output
    of annotateintervals). If provided, explicit gc correction will be performed before
    performing svd. Intervals must be identical to and in the same order as those
    in the input read-counts files.
  id: annotated_intervals
  inputBinding:
    position: 4
    prefix: --annotated-intervals
    shellQuote: false
  label: Annotated intervals
  sbg:category: Optional Arguments
  sbg:fileTypes: INTERVALS, INTERVAL_LIST, BED, TSV
  sbg:toolDefaultValue: 'null'
  type: File?
- doc: If true, impute zero-coverage values as the median of the non-zero values in
    the corresponding interval. (this is applied after all filters.).
  id: do_impute_zeros
  inputBinding:
    position: 4
    prefix: --do-impute-zeros
    shellQuote: false
  label: Do impute zeros
  sbg:category: Optional Arguments
  sbg:toolDefaultValue: 'true'
  type:
  - 'null'
  - name: do_impute_zeros
    symbols:
    - 'true'
    - 'false'
    type: enum
- doc: Fractional coverages normalized by genomic-interval medians that are below
    this percentile or above the complementary percentile are set to the corresponding
    percentile value. (this is applied after all filters and imputation.) 1.
  id: extreme_outlier_truncation_percentile
  inputBinding:
    position: 4
    prefix: --extreme-outlier-truncation-percentile
    shellQuote: false
  label: Extreme outlier truncation percentile
  sbg:category: Optional Arguments
  sbg:toolDefaultValue: '0'
  type: float?
- doc: Samples with a median (across genomic intervals) of fractional coverage normalized
    by genomic-interval medians below this percentile or above the complementary percentile
    are filtered out. (this is the fourth filter applied.) 5.
  id: extreme_sample_median_percentile
  inputBinding:
    position: 4
    prefix: --extreme-sample-median-percentile
    shellQuote: false
  label: Extreme sample median percentile
  sbg:category: Optional Arguments
  sbg:toolDefaultValue: '2'
  type: float?
- doc: Input tsv or hdf5 files containing integer read counts in genomic intervals
    for all samples in the panel of normals (output of collectreadcounts). Intervals
    must be identical and in the same order for all samples. This argument must be
    specified at least once.
  id: read_counts
  inputBinding:
    position: 4
    prefix: --input
    shellQuote: false
    valueFrom: "${\n    if (self) {\n        self = [].concat(self);\n        var\
      \ paths = [];\n        for (var i=0; i<self.length; i++) {\n            paths.push(self[i].path);\n\
      \        }\n        return paths.join(' --input ');\n    }\n    return '';\n\
      }\n"
  label: Input read counts
  sbg:altPrefix: -I
  sbg:category: Required Arguments
  sbg:fileTypes: HDF5, TSV
  type: File[]
- doc: Maximum hdf5 matrix chunk size. Large matrices written to hdf5 are chunked
    into equally sized subsets of rows (plus a subset containing the remainder, if
    necessary) to avoid a hard limit in java hdf5 on the number of elements in a matrix.
    However, since a single row is not allowed to be split across multiple chunks,
    the number of columns must be less than the maximum number of values in each chunk.
    Decreasing this number will reduce heap usage when writing chunks.
  id: maximum_chunk_size
  inputBinding:
    position: 4
    prefix: --maximum-chunk-size
    shellQuote: false
  label: Maximum chunk size
  sbg:category: Advanced Arguments
  sbg:toolDefaultValue: '16777215'
  type: int?
- doc: Genomic intervals with a fraction of zero-coverage samples above this percentage
    are filtered out. (this is the third filter applied.) 0.
  id: maximum_zeros_in_interval_percentage
  inputBinding:
    position: 4
    prefix: --maximum-zeros-in-interval-percentage
    shellQuote: false
  label: Maximum zeros in interval percentage
  sbg:category: Optional Arguments
  sbg:toolDefaultValue: '5'
  type: float?
- doc: Samples with a fraction of zero-coverage genomic intervals above this percentage
    are filtered out. (this is the second filter applied.) 0.
  id: maximum_zeros_in_sample_percentage
  inputBinding:
    position: 4
    prefix: --maximum-zeros-in-sample-percentage
    shellQuote: false
  label: Maximum zeros in sample percentage
  sbg:category: Optional Arguments
  sbg:toolDefaultValue: '5'
  type: float?
- doc: Memory overhead which will be allocated for one job.
  id: memory_overhead_per_job
  label: Memory overhead per job
  sbg:category: Execution
  sbg:toolDefaultValue: '0'
  type: int?
- doc: Memory which will be allocated for execution.
  id: memory_per_job
  label: Memory per job
  sbg:category: Execution
  sbg:toolDefaultValue: '2048'
  type: int?
- doc: Genomic intervals with a median (across samples) of fractional coverage (optionally
    corrected for gc bias) less than or equal to this percentile are filtered out.
    (this is the first filter applied.) 0.
  id: minimum_interval_median_percentile
  inputBinding:
    position: 4
    prefix: --minimum-interval-median-percentile
    shellQuote: false
  label: Minimum interval median percentile
  sbg:category: Optional Arguments
  sbg:toolDefaultValue: '10'
  type: float?
- doc: Number of eigensamples to use for truncated svd and to store in the panel of
    normals. The number of samples retained after filtering will be used instead if
    it is smaller than this.
  id: number_of_eigensamples
  inputBinding:
    position: 4
    prefix: --number-of-eigensamples
    shellQuote: false
  label: Number of eigensamples
  sbg:category: Optional Arguments
  sbg:toolDefaultValue: '20'
  type: int?
- doc: Output file for the panel of normals.
  id: output_prefix
  label: Output prefix
  sbg:altPrefix: -O
  sbg:category: Required Arguments
  type: string?
- doc: Name of the program running.
  id: program_name
  inputBinding:
    position: 4
    prefix: --program-name
    shellQuote: false
  label: Program name
  sbg:category: Optional Arguments
  sbg:toolDefaultValue: 'null'
  type: string?
- doc: Number of CPUs which will be allocated for the job.
  id: cpu_per_job
  label: CPU per job
  sbg:category: Execution
  sbg:toolDefaultValue: '1'
  type: int?
label: GATK CreateReadCountPanelOfNormals
outputs:
- doc: Panel-of-normals file. This is an HDF5 file containing the panel data in the
    paths defined in HDF5SVDReadCountPanelOfNormals
  id: panel_of_normals
  label: Panel of normals
  outputBinding:
    glob: '*pon.hdf5'
    outputEval: $(inheritMetadata(self, inputs.read_counts))
  sbg:fileTypes: HDF5
  type: File?
requirements:
- class: ShellCommandRequirement
- class: ResourceRequirement
  coresMin: '$(inputs.cpu_per_job ? inputs.cpu_per_job : 1)'
  ramMin: "${  \n    var memory = 2048;\n    if (inputs.memory_per_job) {\n      \
    \  memory = inputs.memory_per_job;\n    }\n    if (inputs.memory_overhead_per_job)\
    \ {\n        memory += inputs.memory_overhead_per_job;\n    }\n    return memory;\n\
    }"
- class: DockerRequirement
  dockerPull: images.sbgenomics.com/stefan_stojanovic/gatk:4.1.0.0
- class: InitialWorkDirRequirement
  listing: []
- class: InlineJavascriptRequirement
  expressionLib:
  - "var updateMetadata = function(file, key, value) {\n    file['metadata'][key]\
    \ = value;\n    return file;\n};\n\n\nvar setMetadata = function(file, metadata)\
    \ {\n    if (!('metadata' in file))\n        file['metadata'] = metadata;\n  \
    \  else {\n        for (var key in metadata) {\n            file['metadata'][key]\
    \ = metadata[key];\n        }\n    }\n    return file\n};\n\nvar inheritMetadata\
    \ = function(o1, o2) {\n    var commonMetadata = {};\n    if (!Array.isArray(o2))\
    \ {\n        o2 = [o2]\n    }\n    for (var i = 0; i < o2.length; i++) {\n   \
    \     var example = o2[i]['metadata'];\n        for (var key in example) {\n \
    \           if (i == 0)\n                commonMetadata[key] = example[key];\n\
    \            else {\n                if (!(commonMetadata[key] == example[key]))\
    \ {\n                    delete commonMetadata[key]\n                }\n     \
    \       }\n        }\n    }\n    if (!Array.isArray(o1)) {\n        o1 = setMetadata(o1,\
    \ commonMetadata)\n    } else {\n        for (var i = 0; i < o1.length; i++) {\n\
    \            o1[i] = setMetadata(o1[i], commonMetadata)\n        }\n    }\n  \
    \  return o1;\n};\n\nvar toArray = function(file) {\n    return [].concat(file);\n\
    };\n\nvar groupBy = function(files, key) {\n    var groupedFiles = [];\n    var\
    \ tempDict = {};\n    for (var i = 0; i < files.length; i++) {\n        var value\
    \ = files[i]['metadata'][key];\n        if (value in tempDict)\n            tempDict[value].push(files[i]);\n\
    \        else tempDict[value] = [files[i]];\n    }\n    for (var key in tempDict)\
    \ {\n        groupedFiles.push(tempDict[key]);\n    }\n    return groupedFiles;\n\
    };\n\nvar orderBy = function(files, key, order) {\n    var compareFunction = function(a,\
    \ b) {\n        if (a['metadata'][key].constructor === Number) {\n           \
    \ return a['metadata'][key] - b['metadata'][key];\n        } else {\n        \
    \    var nameA = a['metadata'][key].toUpperCase();\n            var nameB = b['metadata'][key].toUpperCase();\n\
    \            if (nameA < nameB) {\n                return -1;\n            }\n\
    \            if (nameA > nameB) {\n                return 1;\n            }\n\
    \            return 0;\n        }\n    };\n\n    files = files.sort(compareFunction);\n\
    \    if (order == undefined || order == \"asc\")\n        return files;\n    else\n\
    \        return files.reverse();\n};"
  - "\nvar setMetadata = function(file, metadata) {\n    if (!('metadata' in file))\n\
    \        file['metadata'] = metadata;\n    else {\n        for (var key in metadata)\
    \ {\n            file['metadata'][key] = metadata[key];\n        }\n    }\n  \
    \  return file\n};\n\nvar inheritMetadata = function(o1, o2) {\n    var commonMetadata\
    \ = {};\n    if (!Array.isArray(o2)) {\n        o2 = [o2]\n    }\n    for (var\
    \ i = 0; i < o2.length; i++) {\n        var example = o2[i]['metadata'];\n   \
    \     for (var key in example) {\n            if (i == 0)\n                commonMetadata[key]\
    \ = example[key];\n            else {\n                if (!(commonMetadata[key]\
    \ == example[key])) {\n                    delete commonMetadata[key]\n      \
    \          }\n            }\n        }\n    }\n    if (!Array.isArray(o1)) {\n\
    \        o1 = setMetadata(o1, commonMetadata)\n    } else {\n        for (var\
    \ i = 0; i < o1.length; i++) {\n            o1[i] = setMetadata(o1[i], commonMetadata)\n\
    \        }\n    }\n    return o1;\n};"
sbg:appVersion:
- v1.0
sbg:categories:
- Genomics
- Copy Number Variant Calling
sbg:content_hash: a8fa1402a4f277d183dc881323c7d175a62dc05bae4186725747f3ec9aecfb8b2
sbg:contributors:
- uros_sipetic
- stefan_stojanovic
sbg:copyOf: veliborka_josipovic/gatk-4-1-0-0-toolkit-dev/gatk-createreadcountpanelofnormals-4-1-0-0/19
sbg:createdBy: uros_sipetic
sbg:createdOn: 1553104145
sbg:id: uros_sipetic/gatk-4-1-0-0-demo/gatk-createreadcountpanelofnormals-4-1-0-0/2
sbg:image_url: null
sbg:latestRevision: 2
sbg:license: Open source BSD (3-clause) license
sbg:modifiedBy: stefan_stojanovic
sbg:modifiedOn: 1559645270
sbg:project: uros_sipetic/gatk-4-1-0-0-demo
sbg:projectName: GATK 4.1.0.0 - Demo
sbg:publisher: sbg
sbg:revision: 2
sbg:revisionNotes: Copy of veliborka_josipovic/gatk-4-1-0-0-toolkit-dev/gatk-createreadcountpanelofnormals-4-1-0-0/19
sbg:revisionsInfo:
- sbg:modifiedBy: uros_sipetic
  sbg:modifiedOn: 1553104145
  sbg:revision: 0
  sbg:revisionNotes: Copy of veliborka_josipovic/gatk-4-1-0-0-toolkit-dev/gatk-createreadcountpanelofnormals-4-1-0-0/9
- sbg:modifiedBy: stefan_stojanovic
  sbg:modifiedOn: 1553864213
  sbg:revision: 1
  sbg:revisionNotes: Copy of veliborka_josipovic/gatk-4-1-0-0-toolkit-dev/gatk-createreadcountpanelofnormals-4-1-0-0/18
- sbg:modifiedBy: stefan_stojanovic
  sbg:modifiedOn: 1559645270
  sbg:revision: 2
  sbg:revisionNotes: Copy of veliborka_josipovic/gatk-4-1-0-0-toolkit-dev/gatk-createreadcountpanelofnormals-4-1-0-0/19
sbg:sbgMaintained: false
sbg:toolAuthor: Broad Institute
sbg:toolkit: GATK
sbg:toolkitVersion: 4.1.0.0
sbg:validationErrors: []
